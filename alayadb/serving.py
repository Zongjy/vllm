from vllm import LLM, SamplingParams
from rich.console import Console
from rich.table import Table
from rich.prompt import Prompt
from rich import box
import logging
import os
from datetime import datetime
import sys

# åˆ›å»ºæ—¥å¿—ç›®å½•
os.makedirs("logs", exist_ok=True)
os.environ["VLLM_USE_V1"] = "1"
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

# é…ç½®åŸºç¡€æ—¥å¿—
log_filename = datetime.now().strftime("logs/vllm_%Y%m%d_%H%M%S.log")
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(log_filename, encoding="utf-8"),
        logging.StreamHandler(sys.stdout),
    ],
)

# è·å– vLLM ä¸“ç”¨æ—¥å¿—å™¨å¹¶è®¾ç½®çº§åˆ«
vllm_logger = logging.getLogger("vllm")
vllm_logger.setLevel(logging.DEBUG)  # æ›´è¯¦ç»†çš„æ—¥å¿—çº§åˆ«

# åˆå§‹åŒ– rich console
console = Console()

model_name = "Qwen/Qwen2.5-0.5B-Instruct"


# åŠ è½½æœ¬åœ°æ¨¡å‹
console.print("[bold magenta]ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹...[/bold magenta]")
vllm_logger.info(
    f"å¼€å§‹åŠ è½½æ¨¡å‹ï¼š./{model_name}"  # noqa: G004
)  # è®°å½•æ¨¡å‹åŠ è½½æ—¥å¿—  # noqa: G004

try:
    llm = LLM(
        model=f"{model_name}", max_model_len=16384, gpu_memory_utilization=0.5
    )
    vllm_logger.info("æ¨¡å‹åŠ è½½æˆåŠŸ")
except Exception as e:
    log_str = f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
    vllm_logger.critical(log_str)
    console.print("[bold red]âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„[/bold red]")
    sys.exit(1)

console.print(
    f"[bold green]âœ… æ¨¡å‹åŠ è½½å®Œæˆ![/bold green] [dim](æ—¥å¿—æ–‡ä»¶: \
          {log_filename})[/dim]\n"
)

# è®¾ç½®ç”Ÿæˆå‚æ•°
sampling_params = SamplingParams(temperature=0.8, max_tokens=200)
log_str = f"åˆå§‹åŒ–é‡‡æ ·å‚æ•°: {sampling_params}"
vllm_logger.debug(log_str)

# åˆ›å»ºå¯¹è¯å†å²è¡¨æ ¼
history_table = Table(
    title="[bold cyan]ğŸ¤– å¯¹è¯å†å²[/bold cyan]",
    box=box.ROUNDED,
    header_style="bold blue",
    show_lines=True,
    expand=True,
)
history_table.add_column("è½®æ¬¡", width=8)
history_table.add_column("é—®é¢˜", width=40)
history_table.add_column("å›ç­”", width=80)

# ...åç»­ä»£ç ä¿æŒä¸å˜...

round_count = 1

# äº¤äº’å¾ªç¯
while True:
    try:
        # ç¾åŒ–è¾“å…¥æç¤º
        prompt = Prompt.ask(
            "[bold yellow]ğŸ’¬ è¯·è¾“å…¥ä½ çš„é—®é¢˜[/bold yellow] (è¾“å…¥ 'exit' é€€å‡º)",
            console=console,
        )

        if prompt.__len__() == 0:
            console.print("[red]âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜")
            continue

        if prompt.lower() in ("exit", "quit"):
            console.print("[bold magenta]\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼[/bold magenta]")
            break

        # ç”Ÿæˆå›ç­”
        with console.status("[bold green]ç”Ÿæˆä¸­..."):
            outputs = llm.generate([prompt], sampling_params)

        # æå–ç»“æœ
        answer = outputs[0].outputs[0].text.strip()
        for j in range(0, len(outputs)):
            for i in range(1, len(outputs[0].outputs)):
                answer += f"\n{outputs[j].outputs[i].text.strip()}"

        # æ›´æ–°å¯¹è¯å†å²è¡¨æ ¼
        history_table.add_row(
            str(round_count),
            f"[italic]{prompt}[/italic]",
            f"[green]{answer}[/green]",
        )

        # æ‰“å°æœ¬æ¬¡å›ç­”
        console.print("\n[bold cyan]ğŸ“ æœ¬æ¬¡å›ç­”ï¼š[/bold cyan]")
        console.print(f"[green]{answer}[/green]\n")

        # æ‰“å°å®Œæ•´å¯¹è¯å†å²
        console.print(history_table)
        round_count += 1

    except EOFError:
        console.print("[bold magenta]\nğŸ‘‹ è¯»å–åˆ°EOFï¼Œç¨‹åºé€€å‡º[/bold magenta]")
        break
    except KeyboardInterrupt:
        console.print("[bold magenta]\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º[/bold magenta]")
        break
